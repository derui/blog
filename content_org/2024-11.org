#+startup: content logdone inlneimages

#+hugo_base_dir: ../
#+hugo_section: post/2024/11
#+author: derui

* DONE Ollamaをnixos上で使えるようにしてみる :Linux:
CLOSED: [2024-11-02 土 13:46]
:PROPERTIES:
:EXPORT_FILE_NAME: use-ollama-on-nixos
:END:
やっと秋めいてきましたが、なんか台風がきたりと落ち着かない感じですね。

今回NixOSに移行したことで、色々やっても戻せるという確信が大体ついたので、Local LLMに手を出してみました。

#+html: <!--more-->

** Ollamaとは
https://github.com/ollama/ollama

簡単に書くと、 *LLMのmodelを切り替えながら統一したinterfaceで実行できる環境* を提供するツールです。元々はLlamaというMetaが出しているmodelを利用するために作られたみたいですが、今では[[https://ollama.com/library][Model library]]というところからダウンロードして使えるようになっています。

通常だとCPUのみを利用するので、結構速度に課題がありますが、NVidiaやAMDのGPUを利用して高速に実行させることもできます。

特徴としては、 *複数のmodelを同時に実行する* ことができます。メモリさえ許せば、になりますが・・・。

** NixOSでOllamaを使う
nixなので、Ollamaを利用するだけなら簡単です（ほんとに？）。

#+begin_src nix
  { pkgs, ... }:
  {
    # AMDのCUDA的な立ち位置のrocmを有効にする
    environment.systemPackages = [ pkgs.ollama-rocm ];
    services.ollama = {
      # 設定を有効にする。service == systemdのunitとして起動するようになる
      enable = true;
      # これを指定しないとCPUだけで実行する形になる
      acceleration = "rocm";
      group = "ollama";

      environmentVariables = {
        HCC_AMDGPU_TARGET = "gfx1100"; # used to be necessary, but doesn't seem to anymore
      };
      rocmOverrideGfx = "11.0.0";
    };
  }
#+end_src

私は [[https://www.amd.com/ja/products/graphics/desktops/radeon/7000-series/amd-radeon-rx-7900xtx.html][Radeon RX 7900 XTX]]を利用しているので、rocmを利用することができるみたいだったので、rocmを有効にしています。これだけで、systemdでunitが設定されて、自動的に起動します。 ~ollama~ のcommandも使えるようになるので、modelをダウンロードすることもできます。

GPUを利用するように設定すると、CPUと比較しても10倍程度高速になります。GPUの消費電力はえげつないくらい上がりますが（一瞬で300Wとかに到達する）、これくらいないと正直待ってる間に他のことができてしまうレベルですね。

ちなみにmodelとしてはgemma2を主に利用しています。providerを都度切り替えるのがめんどくさいので。

** EmacsからOllamaを使う
私は大体Emacsの上で色々やってますので、Emacsから使えないかを検討する必要がありました。さしあたって調べると、majorなものとしては以下がありそうでした。

- [[https://github.com/s-kostyaev/ellama?tab=readme-ov-file][ellama]]
  - LLMに対するfrontendとして設計されたもの
  - *chatよりも、特定の作業をさせること* を念頭に置いたinterface
- [[https://github.com/karthink/gptel][gptel]]
  - 様々なLLMとchatするためのinterfaceを起点としたinterface
  - contextの設定とかを色々できるが、基本的にはchat clientである


私としてはchatはめんどくさいので、ellamaを利用しています。ellama自体transientのmenuを持ったりしていますが、個人的に全部入りのmenuはどうせ使わないので、以下のように自前で定義しています。

#+begin_src emacs-lisp
  (transient-define-prefix my:llm-transient ()
    "Menus with LLM"
    [
     ["Code related"
      ("c" "Complete code" ellama-code-complete :transient nil)
      ("a" "Add code" ellama-code-add :transient nil)
      ("i" "Improve code" ellama-code-improve :transient nil)
      ("e" "Edit code" ellama-code-edit :transient nil)
      ("R" "Review code" ellama-code-review :transient nil)
      ]
     ["Generic usage"
      ("s" "Summary selected content or buffer" ellama-summarize :transient nil)
      ("t" "Translate selected content" ellama-translate :transient nil)
      ]
     ["Grammar"
      ("W" "Improve wording" ellama-improve-wording :transient nil)
      ("I" "Improve grammer" ellama-improve-grammar :transient nil)
      ]]
    [["Misc"
      ("*" "Open chat" ellama-chat :transient t)
      ]]
    )
#+end_src

使っていてだいぶ癖があるのが、 ~ellama-code-add~ です。利用すると、ほとんどのケースで先頭から書き直そうとして大変邪魔という・・・。正直使い勝手がわからないというのが現状の感覚ですね。chatは連続することがあるので、transientを切らないようにしています。

** で、Local LLMはどうなの？
正直あんまり使ってないのでなんともですが、置換だと難しいような箇所を変換する、というのが主な感じです。test caseの生成とかについては、生成するために色々頑張るより自分でcopy/pasteして書き直したほうが早いというのがなんともですね。

Emacs上だとcopilotとか使ってても邪魔すぎて（overlayなので）、現時点では切っているくらいなので。実は [[https://github.com/bernardo-bruning/ollama-copilot]] という、ollamaをcopilotとして（かなり無理矢理）使う、というツールも見つけたので、これを使ってみようともしました。
が、Emacsからだと *ollama-copilotを使うためにGitHub Copilotにloginしないといけない* というなんともかんともな感じになってしまったので、今回記事にはしていません。またなんか進捗があれば書こうかなと思います。

* DONE tabbyをvulkanで使えるようにしてみた :Linux:
CLOSED: [2024-11-14 木 23:26]
:PROPERTIES:
:EXPORT_FILE_NAME: tabby-with-vulkan
:END:
暖かくなったり寒くなったりと格好が定まりませんね。

さて、最近tabbyというツールを知ったので、NixOS上で使ってみました。

#+html: <!--more-->


** tabbyとは
https://tabby.tabbyml.com/docs/welcome/

これです。

#+begin_quote
Tabby is an open-source, self-hosted AI coding assistant. With Tabby, every team can set up its own LLM-powered code completion server with ease.
#+end_quote

引用すると、self-hostedなAI coding assistantということで、self-hostedなGitHub Copilotといったところでしょうか。特徴として、

- 全体がオープンソース
  - enterpriseという形でlicenceを発行してはいますが、enterprise向けの内容もrepositoryに入ってます
- server/frontも全部入り
  - Ollamaはbackendだけですが、VS Code Extensionやllama.cppを利用したmodel servingなどもincludedです
  - frontendとしてSPAが用意されていて、補完の回数とかaccept rateとかが見られます


ということでいつもの通りnixpkgsで調べたところ、見つけたので入れてみました。ていうかnixpkgsなんでもありすぎだろう。

** buildが通らない
https://github.com/NixOS/nixpkgs/blob/nixos-unstable/pkgs/by-name/ta/tabby/package.nix

ところが、rocmを指定したところ、そもそもビルドが通りません :cry: どうも、tabbyはllama.cppのRust bindingを利用しているようなのですが、cargo buildのタイミングでllama.cpp自体をビルドするようです。このとき、CMakeが必要なんですが、これが足りてませんでした。
（手元にRadeonなりがないとどうしても確認ができないので、こういうことはよくある印象です）

ただ、追加しても今度はrocm自体の依存が不足しているようでした。諸々調べると、 *そもそもllama.cppはすでにビルドしているんだからそれ使おうよ* というアプローチが取られていました（それでもbuildは失敗するのですが :thinking:）。

** CPUしか使ってくれない
Rocmを切るとbuildは通るのですが、まーCPUだけだと32 coreのCPUですら使い物になるか怪しいくらいの性能しか出ないです。llama.cppは実はvulkan APIを利用したGPU offloadがサポートされているのですが、nixpkgsにmergeされている版だと対応されていません。

こうなってくると、いつものようにoverlayを作成する形になります。

https://github.com/derui/my-nixos/blob/main/overlays/tabby/package.nix

ただ、tabbyは通常serviceとして動かすようなものっぽいので、できればserviceも使いたいです。が、nixpkgsのserviceではvulkanを指定することができません。

#+begin_quote
      acceleration = lib.mkOption {
        # ↓vulkanがない
        type = types.nullOr (types.enum [ "cpu" "rocm" "cuda" "metal" ]);
        default = null;
        example = "rocm";
        description = ''
          Specifies the device to use for hardware acceleration.

          -   `cpu`: no acceleration just use the CPU
          -  `rocm`: supported by modern AMD GPUs
          -  `cuda`: supported by modern NVIDIA GPUs
          - `metal`: supported on darwin aarch64 machines

          Tabby will try and determine what type of acceleration that is
          already enabled in your configuration when `acceleration = null`.

          - nixpkgs.config.cudaSupport
          - nixpkgs.config.rocmSupport
          - if stdenv.hostPlatform.isDarwin && stdenv.hostPlatform.isAarch64

          IFF multiple acceleration methods are found to be enabled or if you
          haven't set either `cudaSupport or rocmSupport` you will have to
          specify the device type manually here otherwise it will default to
          the first from the list above or to cpu.
        '';
      };
#+end_quote

となると、Serviceもoverlayする感じになります。が、これのやり方が大分わかりません。色々調べてみたところ、公式としては ~disabledModules~ を利用しろ、とのことでした。

#+begin_src nix
    # disabled original module
    disabledModules = [ "services/misc/tabby.nix" ];
#+end_src

指定するpathはstoreからの相対pathなどを指定できます。これを指定したあと、moduleを再定義することで、カスタマイズしたmoduleを利用できます。

** tabbyのclient
いつもの如く、Emacsからしか使わないので、Emacsのtabby用clientを導入しています。

https://github.com/alan-w-255/tabby.el

大体はcopilot.elと同様の使い勝手です。vulkanで利用していますが、大体0.3秒位で補完されるので、そこまでストレスではないです。ただ、どんだけresourceを突っ込んでいるかわからないGitHub Copilotとかと比較すると、精度としてはいまいちと感じることが多いです。あくまで補助といったところですね。

** Ollamaをbackendにすることもできる
設定をすることで、Ollamaを利用してcompletionとかを行うこともできます。modelの自由度はこっちのほうが圧倒的に高いので、量子化されたmodelを使いたい、とかの場合は検討してみてはいかがでしょうか。NixOSならRocmを有効にしてbuildできますよ。

* comment Local Variables                                           :ARCHIVE:
# Local Variables:
# eval: (org-hugo-auto-export-mode)
# End:

