#+startup: content logdone inlneimages

#+hugo_base_dir: ../
#+hugo_section: post/2024/08
#+author: derui

* DONE Kanzenという日本語入力の方式を再実装してみた Part.1 :Emacs:
CLOSED: [2024-08-03 土 11:44]
:PROPERTIES:
:EXPORT_FILE_NAME: chokan_part_1
:END:
夏まっ盛りというところですが、いかがお過しでしょうか。暑いのは中々避けようがないですね。

何回かに分けて、現在自分で利様している日本語入力方式について書いていこうと思います。今回はその1ということで、概要からとなります。

#+html: <!--more-->

** Kanzenではないkanzen - chokan
https://github.com/derui/chokan

repositoryとしては上記となっています。まず名前としては、 ~chotto-kanzen~ ということで命名しています。ちょっとだけ、なのは、元々のkanzenで実装されていた機能をカバーしきっていないためです。


** Kanzenとは
https://www.nue.org/nue/tao/kanzen/wdj.html

Kanzenは、漢字変換システムの一種です。Kanzen自体の特徴としては、

- *第一文節までを自動的に変換する*
- *文節の区切りは自分で決定する*
- *品詞解析を積極的に行い、質の高い変換を行えるようにする*


というものがあります。文節区切りのコントロールをユーザーが行う点などは、SKKと類似していると感じる方もいると思いますが、これは偶然ではなく、SKKの元々の開発者がKanzenを参考にしたと書かれています。

https://ja.wikipedia.org/wiki/SKK

*** 再実装を試みたのか
元々はSKKを使っていたのですが、ふとしたことでかな入力に戻したのです。そのとき改めて、、SKKの課題である *QWERTYに仕様が最適化されすぎていて、かな入力などでは使いずらい* というのが利用上のネックとなりました。

#+begin_quote
chokanでも完全に対策できているというわけではないのですが。
#+end_quote

なにかしら方策がないかな、と探していたところ、SKKが参考にしたというKanzenを発見し、 *新JISに対応できている* ということから、実装してみようとなった次第です。

** chokanのアーキテクチャ
オリジナルのKanzenは、1986年（！）に発表されていることもあり、そもそもEmacsですらなく、TAO/ELISというシステムで動作していたEmacsライクなeditorであるZenで動作するように開発されていました。

元々zenというeditorには、構文解析のフロントエンドが実装されていたようですが、仕組みとしては、ZenからELISのマイクロプログラムを呼び出す、という方式だったようです。今でいうと外部プロセスの起動か、内部の関数呼び出しに相当するというところでしょうか。
chokanでも同様にemacs上ですべて実装することもできましたが、ロジックの再利用性を一定考慮して、次のようにしています。

- frontend :: Emacs
  - これはシンプルに実装コストとの兼ね合いからEmacsだけ実装したためです
- backend :: Rust
- network protocol :: Websocket
- front-to-back interface :: JSON-RPC


Rustで構築されたJSON-RPCサーバーとフロントエンドが、Websocketで通信する、というモデルにしています。JSON-RPCを選択したのは、EmacsがLSP対応(Eglotによる)を公式に行ったことにより、emacsからのWebsocket/JSON-RPC実行が非常にやりやすくなったためです。

#+begin_quote
当初はprotobufなどでなんとか生成したり・・・とか考えてましたが、Emacs向けの実装とかを考えると、もっとシンブルでいいじゃん、ということに気付いたので
#+end_quote

*** 辞書の管理
chokanでは *品詞解析が必要* ということから、当然辞書が必要になります。辞書の形式としてはSKKを参考にしつつ、品詞を設定することが出来るようにしています。

#+begin_example
  はがゆ	歯痒	/形容詞/
  じゅん	殉	/ザ行上一/
#+end_example

~<read><tab><word><tab>/品詞(/...)/~ となっています。同一の読み、単語で複数の品詞、というのも存在しうるので、それを表現できるようになっています。

辞書は大きく *標準* *付属* *ユーザー* の3種類があり、用途が異なります。

** 性能について
日本語入力ということで、当然ですが既存のシステムと比較されるのは当然ではあると思います。が、そもそもの思想が異なるため、同一条件における比較は難しいので、現状の主観での評価を書いておきます。

*** 実行速度
かな漢字変換では、当然ですが変換にかかる時間がどれだけ短いか？というのも体験として重要です。chokanは、Rustで実装した/解析が限定されている、ということもあり、 *候補が少ない場合は1～2ms、多い場合でも3～6ms* という時間で候補を出すことができています。

Emacs側の実装でも、そこまで複雑なことをしないようにしているので、mozc + overlayのようなチラつきなどはほぼ発生せず、快適に利用できるようにできています。

*** 変換精度
chokanは第一文節のみを考慮して変換しているため、SKKというよりはGoogle日本語入力などと感覚は似ています。
しかし、区切りの指定があるものの、SKKと違って変換の区切りを指定しているというわけではないため、SKKよりはShiftなりの頻度は低くできます。

ただ、品詞まで考慮しているものの、どうしても現代的なATOK/Google日本語入力などによる精度を出すことは難しいです。特に1文字単語などにおける煩雑さは、SKKと類似しています。

#+begin_quote
現代の日本語入力は、ほぼほぼ統計的なやりかたになっていて、下手に品詞解析するよりもそっちの方が性能が出るとのことです。莫大なデータが必要になるので、個人では大分難しいですね。
#+end_quote



** next
chokanの概要について紹介しました。次は利用しているアルゴリズムなどを書いていこうと思います。

* DONE Kanzenという日本語入力の方式を再実装してみた Part.2 :Rust:
CLOSED: [2024-08-15 木 17:56]
:PROPERTIES:
:EXPORT_FILE_NAME: chokan_part_2
:END:
盆の前後が台風という、なかなかな感じですが、元気にやっていきたいところですね。

前回に引き続き、chokanについて書いていきます。今回は辞書で利用しているアルゴリズムと、実装方針について書いていきます。

html: <!--more-->

** 辞書引きのデータ構造 - Trie
日本語入力に限らず、なんらかの辞書から対象のエントリーを探すとき、[[https://ja.wikipedia.org/wiki/%E3%83%88%E3%83%A9%E3%82%A4_(%E3%83%87%E3%83%BC%E3%82%BF%E6%A7%8B%E9%80%A0)][Trie]]というデータ構造がよく使われます。

Trieは一般に以下の特徴を持ちます。

- 文字列を表現することに非常に適している
- 同じprefixは共有されるため、メモリ効率がよい
- 2分探索木と比較して、高速である場合がおおい
- 最長一致マッチが自然に表現できる


これらの特徴から、よく辞書の内部表現として利用されています。chokanでも特に理由はないので、これを採用しています。

** Trieの実装 - ダブル配列
Trieは単なる構造の定義なので、実際の実装としては様々なものが考えられます。文字列に対する構造としては、2重連鎖木（以前migemoを実装したときに触れました）での実装などがあります。

その中でも特筆すべき構造として *ダブル配列* があります。これは ~base~ / ~check~ / ~next~ というものを2本の配列で表現することで、 ~O(1)~ でのNode間遷移を可能としています。

より詳しい説明はWikipediaやSlideshareなどにありますので、興味があれば探してみてください。

なぜ遷移の時間が少ないものがよいのか？という点については、


- 日本語入力では都度入力に対応するnodeを探す必要がある
- 2重連鎖木などでは、実装においてはポインターを辿る必要がある
- prefixを辿る度にポインターを辿るのは、どうしても時間がかかる


という、実機における制約があるためです。特に大量（10万単位でのエントリー）のprefixを都度探索することを考えると、高速であるというのは非常に重要な要素となります。

** ダプル配列の難しさ
さて、使うだけならいいことづくめに見えるダブル配列ですが、実装してみると *追加・削除がとてもややこしい* というものがあります。そのため、普通はライブラリーを利用するのが普通なのですが、chokanでは *アルゴリズムに関するものは自分で作る* という方針でやっているため、Trieについても自前で実装しています。

https://github.com/derui/chokan/blob/315c2542db0a117fccd3e0fe111c8e6a73f5070a/libs/trie/src/lib.rs

上で一通り実装しています。動的な追加はサポートしないとユーザー辞書との結合ができないかな、と思ったので実装しています。が、動的な削除は、その難易度に比べて利用シーンが思いうかばなかったので、実装を見送っています。

ちなみに、ライブラリによっては *静的な構築のみ* サポートしている、というものもあります。これは、初期構築のみであれば、データを辞書順にならべておくことで、動的な追加における決定的な非効率性を避けることができるためです。
実際、ベースとなる辞書は10万というオーダーでエントリーが存在していたとして、ユーザー辞書がその規模になることはほとんどありません。また、ベース辞書は配布するのが基本なので、一回生成するだけでよいです。

が、chokanは辞書の構築方法が特殊なので、配布という方法がとれなかったのと、内部的にベースとユーザー辞書を（今は）マージして扱っている都合で、動的な追加が必要となっています。

実際、こんな感じになっています。一部をコメントアウトしているのは、難易度がかなり高かったので、簡単な方に日和った結果です。xcheckというのは、ダブル配列をTrieに適用した元論文から取られています。

#+begin_src rust
      fn move_conflicted(&mut self, node: &NodeIdx, label: &Label) {
          // 移動する対象を確定する
          let detect_to_move_base: (NodeIdx, Vec<Label>);

          // できるだけ遷移先が少ない方を移動する
          {
              let check_idx = self.nodes.base_of(node).expect("should be a base") + *label;
              let conflicted_node: NodeIdx = self
                  .nodes
                  .check_of(&check_idx)
                  .expect("Can not get check")
                  .into();

              assert!(*node != conflicted_node, "each node should be different");

              let mut current_labels = self.nodes.find_labels_of(node, &self.labels);
              let _conflicted_labels = self.nodes.find_labels_of(&conflicted_node, &self.labels);
              current_labels.push(*label);

              // conflictしたlabel自体も対象に加えて、移動する方を決定する
              // TODO: ここで自分自身のnodeを動かさないと、自分自身のnode自体がずれてしまう。
              // if current_labels.len() < conflicted_labels.len() {
              detect_to_move_base = (*node, current_labels);
              // } else {
              //     detect_to_move_base = (conflicted_node, conflicted_labels);
              // }
          }

          // 全体が入る先を探索して、移動が必要なものを移動する
          let new_base = self.nodes.xcheck(&detect_to_move_base.1);
          self.nodes
              .rebase(&detect_to_move_base.0, &new_base, &self.labels);
      }

#+end_src

ちなみに速度ですが、現状ベース辞書でおよそ10万ちょっとのエントリーがありますが、これを構築するのに [[https://www.amd.com/ja/products/processors/desktops/ryzen/7000-series/amd-ryzen-9-7900x.html][AMD Ryzen 7900X]] で大体 *1分ちょっと* かかります。これを長いととるかどうかですが、実際にユーザー辞書をマージするときは一瞬なので、実用上は問題ないと判断しています。

** next
辞書関連はデータをどう持つか？というのと、 *どうやって辞書のデータを集めるのか* という、ある意味ではこっちの方が難しい問題があります。次回は辞書のデータについて書こうと思います。

* comment Local Variables                                           :ARCHIVE:
# Local Variables:
# eval: (org-hugo-auto-export-mode)
# End:

